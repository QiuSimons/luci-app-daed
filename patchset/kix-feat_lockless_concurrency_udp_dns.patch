diff --git a/component/dns/dns.go b/component/dns/dns.go
index 9800416..3853f8d 100644
--- a/component/dns/dns.go
+++ b/component/dns/dns.go
@@ -25,8 +25,7 @@ var ErrBadUpstreamFormat = fmt.Errorf("bad upstream format")
 type Dns struct {
 	log              *logrus.Logger
 	upstream         []*UpstreamResolver
-	upstream2IndexMu sync.Mutex
-	upstream2Index   map[*Upstream]int
+	upstream2Index   sync.Map // 使用sync.Map替代mutex+map，减少锁竞争
 	reqMatcher       *RequestMatcher
 	respMatcher      *ResponseMatcher
 }
@@ -41,10 +40,10 @@ type NewOption struct {
 func New(dns *config.Dns, opt *NewOption) (s *Dns, err error) {
 	s = &Dns{
 		log: opt.Logger,
-		upstream2Index: map[*Upstream]int{
-			nil: int(consts.DnsRequestOutboundIndex_AsIs),
-		},
+		// upstream2Index 使用sync.Map，无需初始化
 	}
+	// 设置默认的nil映射
+	s.upstream2Index.Store((*Upstream)(nil), int(consts.DnsRequestOutboundIndex_AsIs))
 	// Parse upstream.
 	upstreamName2Id := map[string]uint8{}
 	for i, upstreamRaw := range dns.Upstream {
@@ -73,9 +72,7 @@ func New(dns *config.Dns, opt *NewOption) (s *Dns, err error) {
 						}
 					}
 
-					s.upstream2IndexMu.Lock()
-					s.upstream2Index[upstream] = i
-					s.upstream2IndexMu.Unlock()
+					s.upstream2Index.Store(upstream, i)
 					return nil
 				}
 			}(i),
@@ -207,9 +204,11 @@ func (s *Dns) ResponseSelect(msg *dnsmessage.Msg, fromUpstream *Upstream) (upstr
 		}
 	}
 
-	s.upstream2IndexMu.Lock()
-	from := s.upstream2Index[fromUpstream]
-	s.upstream2IndexMu.Unlock()
+	fromValue, ok := s.upstream2Index.Load(fromUpstream)
+	if !ok {
+		fromValue = int(consts.DnsRequestOutboundIndex_AsIs) // 默认值
+	}
+	from := fromValue.(int)
 	// Route.
 	upstreamIndex, err = s.respMatcher.Match(qname, qtype, ips, consts.DnsRequestOutboundIndex(from))
 	if err != nil {
diff --git a/control/anyfrom_pool.go b/control/anyfrom_pool.go
index 226e55f..668fcab 100644
--- a/control/anyfrom_pool.go
+++ b/control/anyfrom_pool.go
@@ -161,71 +161,77 @@ func appendUDPSegmentSizeMsg(b []byte, size uint16) []byte {
 
 // AnyfromPool is a full-cone udp listener pool
 type AnyfromPool struct {
-	pool map[string]*Anyfrom
-	mu   sync.RWMutex
+	pool sync.Map // 使用sync.Map减少锁竞争
 }
 
 var DefaultAnyfromPool = NewAnyfromPool()
 
 func NewAnyfromPool() *AnyfromPool {
-	return &AnyfromPool{
-		pool: make(map[string]*Anyfrom, 64),
-		mu:   sync.RWMutex{},
-	}
+	return &AnyfromPool{}
 }
 
 func (p *AnyfromPool) GetOrCreate(lAddr string, ttl time.Duration) (conn *Anyfrom, isNew bool, err error) {
-	p.mu.RLock()
-	af, ok := p.pool[lAddr]
-	if !ok {
-		p.mu.RUnlock()
-		p.mu.Lock()
-		defer p.mu.Unlock()
-		if af, ok = p.pool[lAddr]; ok {
-			return af, false, nil
-		}
-		// Create an Anyfrom.
-		isNew = true
-		d := net.ListenConfig{
-			Control: func(network string, address string, c syscall.RawConn) error {
-				return dialer.TransparentControl(c)
-			},
-			KeepAlive: 0,
-		}
-		var err error
-		var pc net.PacketConn
-		GetDaeNetns().With(func() error {
-			pc, err = d.ListenPacket(context.Background(), "udp", lAddr)
-			return nil
-		})
-		if err != nil {
-			return nil, true, err
-		}
-		uConn := pc.(*net.UDPConn)
-		af = &Anyfrom{
-			UDPConn:       uConn,
-			deadlineTimer: nil,
-			ttl:           ttl,
-			gotGSOError:   false,
-			gso:           isGSOSupported(uConn),
+	if af, ok := p.pool.Load(lAddr); ok {
+		anyfrom := af.(*Anyfrom)
+		anyfrom.RefreshTtl()
+		return anyfrom, false, nil
+	}
+	
+	// 使用双重检查锁定模式避免重复创建
+	// 创建临时key用于创建锁
+	createKey := lAddr + "_creating"
+	if _, loaded := p.pool.LoadOrStore(createKey, struct{}{}); loaded {
+		// 有其他goroutine在创建，等待并重试
+		time.Sleep(time.Microsecond * 100)
+		if af, ok := p.pool.Load(lAddr); ok {
+			anyfrom := af.(*Anyfrom)
+			anyfrom.RefreshTtl()
+			return anyfrom, false, nil
 		}
+	}
+	
+	defer p.pool.Delete(createKey)
+	
+	// 再次检查是否已创建
+	if af, ok := p.pool.Load(lAddr); ok {
+		anyfrom := af.(*Anyfrom)
+		anyfrom.RefreshTtl()
+		return anyfrom, false, nil
+	}
+	
+	// 创建新的Anyfrom
+	d := net.ListenConfig{
+		Control: func(network string, address string, c syscall.RawConn) error {
+			return dialer.TransparentControl(c)
+		},
+		KeepAlive: 0,
+	}
+	var pc net.PacketConn
+	GetDaeNetns().With(func() error {
+		pc, err = d.ListenPacket(context.Background(), "udp", lAddr)
+		return nil
+	})
+	if err != nil {
+		return nil, true, err
+	}
+	
+	uConn := pc.(*net.UDPConn)
+	af := &Anyfrom{
+		UDPConn:       uConn,
+		deadlineTimer: nil,
+		ttl:           ttl,
+		gotGSOError:   false,
+		gso:           isGSOSupported(uConn),
+	}
 
-		if ttl > 0 {
-			af.deadlineTimer = time.AfterFunc(ttl, func() {
-				p.mu.Lock()
-				defer p.mu.Unlock()
-				_af := p.pool[lAddr]
-				if _af == af {
-					delete(p.pool, lAddr)
-					af.Close()
-				}
-			})
-			p.pool[lAddr] = af
-		}
-		return af, true, nil
-	} else {
-		af.RefreshTtl()
-		p.mu.RUnlock()
-		return af, false, nil
+	if ttl > 0 {
+		af.deadlineTimer = time.AfterFunc(ttl, func() {
+			if loaded := p.pool.CompareAndDelete(lAddr, af); loaded {
+				af.Close()
+			}
+		})
 	}
+	
+	p.pool.Store(lAddr, af)
+	return af, true, nil
 }
diff --git a/control/control_plane.go b/control/control_plane.go
index e57cbd8..8ca9bf4 100644
--- a/control/control_plane.go
+++ b/control/control_plane.go
@@ -560,9 +560,16 @@ func (c *ControlPlane) InjectBpf(bpf *bpfObjects) {
 }
 
 func (c *ControlPlane) CloneDnsCache() map[string]*DnsCache {
-	c.dnsController.dnsCacheMu.Lock()
-	defer c.dnsController.dnsCacheMu.Unlock()
-	return deepcopy.Copy(c.dnsController.dnsCache).(map[string]*DnsCache)
+	clonedCache := make(map[string]*DnsCache)
+	c.dnsController.dnsCache.Range(func(key, value interface{}) bool {
+		cache := value.(*DnsCache)
+		// 只有当缓存仍然有效时才克隆，避免无用的深拷贝
+		if cache.Deadline.After(time.Now()) {
+			clonedCache[key.(string)] = deepcopy.Copy(cache).(*DnsCache)
+		}
+		return true
+	})
+	return clonedCache
 }
 
 func (c *ControlPlane) dnsUpstreamReadyCallback(dnsUpstream *dns.Upstream) (err error) {
diff --git a/control/dns_control.go b/control/dns_control.go
index 6a55368..069dd09 100644
--- a/control/dns_control.go
+++ b/control/dns_control.go
@@ -14,7 +14,6 @@ import (
 	"strconv"
 	"strings"
 	"sync"
-	"sync/atomic"
 	"time"
 
 	"github.com/daeuniverse/dae/common/consts"
@@ -62,8 +61,6 @@ type DnsControllerOption struct {
 }
 
 type DnsController struct {
-	handling sync.Map
-
 	routing     *dns.Dns
 	qtypePrefer uint16
 
@@ -76,16 +73,9 @@ type DnsController struct {
 	timeoutExceedCallback func(dialArgument *dialArgument, err error)
 
 	fixedDomainTtl map[string]int
-	// mutex protects the dnsCache.
-	dnsCacheMu          sync.Mutex
-	dnsCache            map[string]*DnsCache
-	dnsForwarderCacheMu sync.Mutex
-	dnsForwarderCache   map[dnsForwarderKey]DnsForwarder
-}
-
-type handlingState struct {
-	mu  sync.Mutex
-	ref uint32
+	// 使用sync.Map代替mutex+map，减少锁竞争
+	dnsCache            sync.Map // map[string]*DnsCache
+	dnsForwarderCache   sync.Map // map[dnsForwarderKey]DnsForwarder
 }
 
 func parseIpVersionPreference(prefer int) (uint16, error) {
@@ -119,11 +109,8 @@ func NewDnsController(routing *dns.Dns, option *DnsControllerOption) (c *DnsCont
 		bestDialerChooser:     option.BestDialerChooser,
 		timeoutExceedCallback: option.TimeoutExceedCallback,
 
-		fixedDomainTtl:      option.FixedDomainTtl,
-		dnsCacheMu:          sync.Mutex{},
-		dnsCache:            make(map[string]*DnsCache),
-		dnsForwarderCacheMu: sync.Mutex{},
-		dnsForwarderCache:   make(map[dnsForwarderKey]DnsForwarder),
+		fixedDomainTtl: option.FixedDomainTtl,
+		// 使用sync.Map，无需初始化
 	}, nil
 }
 
@@ -133,20 +120,15 @@ func (c *DnsController) cacheKey(qname string, qtype uint16) string {
 }
 
 func (c *DnsController) RemoveDnsRespCache(cacheKey string) {
-	c.dnsCacheMu.Lock()
-	_, ok := c.dnsCache[cacheKey]
-	if ok {
-		delete(c.dnsCache, cacheKey)
-	}
-	c.dnsCacheMu.Unlock()
+	c.dnsCache.Delete(cacheKey)
 }
+
 func (c *DnsController) LookupDnsRespCache(cacheKey string, ignoreFixedTtl bool) (cache *DnsCache) {
-	c.dnsCacheMu.Lock()
-	cache, ok := c.dnsCache[cacheKey]
-	c.dnsCacheMu.Unlock()
+	cacheValue, ok := c.dnsCache.Load(cacheKey)
 	if !ok {
 		return nil
 	}
+	cache = cacheValue.(*DnsCache)
 	var deadline time.Time
 	if !ignoreFixedTtl {
 		deadline = cache.Deadline
@@ -287,22 +269,16 @@ func (c *DnsController) __updateDnsCacheDeadline(host string, dnsTyp uint16, ans
 	deadline, originalDeadline := deadlineFunc(now, host)
 
 	cacheKey := c.cacheKey(fqdn, dnsTyp)
-	c.dnsCacheMu.Lock()
-	cache, ok := c.dnsCache[cacheKey]
-	if ok {
-		cache.Answer = answers
-		cache.Deadline = deadline
-		cache.OriginalDeadline = originalDeadline
-		c.dnsCacheMu.Unlock()
-	} else {
-		cache, err = c.newCache(fqdn, answers, deadline, originalDeadline)
-		if err != nil {
-			c.dnsCacheMu.Unlock()
-			return err
-		}
-		c.dnsCache[cacheKey] = cache
-		c.dnsCacheMu.Unlock()
+	
+	// 创建新的缓存项而不是修改现有的，避免数据竞争
+	cache, err := c.newCache(fqdn, answers, deadline, originalDeadline)
+	if err != nil {
+		return err
 	}
+	
+	// 原子性地更新缓存
+	c.dnsCache.Store(cacheKey, cache)
+	
 	if err = c.cacheAccessCallback(cache); err != nil {
 		return err
 	}
@@ -458,18 +434,26 @@ func (c *DnsController) handle_(
 		return c.sendReject_(dnsMessage, req)
 	}
 
-	// No parallel for the same lookup.
-	handlingState_, _ := c.handling.LoadOrStore(cacheKey, new(handlingState))
-	handlingState := handlingState_.(*handlingState)
-	atomic.AddUint32(&handlingState.ref, 1)
-	handlingState.mu.Lock()
-	defer func() {
-		handlingState.mu.Unlock()
-		atomic.AddUint32(&handlingState.ref, ^uint32(0))
-		if atomic.LoadUint32(&handlingState.ref) == 0 {
-			c.handling.Delete(cacheKey)
+	// 使用简化的处理状态管理，避免重复请求
+	handlingState, isNew := GlobalHandlingStateManager.GetOrCreateState(cacheKey)
+	
+	if !isNew {
+		// 有其他goroutine正在处理相同请求，等待完成
+		handlingState.Wait()
+		
+		// 重新检查缓存
+		if resp := c.LookupDnsRespCache_(dnsMessage, cacheKey, false); resp != nil {
+			if needResp {
+				if err = sendPkt(c.log, resp, req.realDst, req.realSrc, req.src, req.lConn); err != nil {
+					return fmt.Errorf("failed to write cached DNS resp: %w", err)
+				}
+			}
+			return nil
 		}
-	}()
+		// 如果仍然没有缓存，继续处理（可能是上一个请求失败了）
+	}
+	
+	defer GlobalHandlingStateManager.CompleteAndCleanup(cacheKey, handlingState)
 
 	if resp := c.LookupDnsRespCache_(dnsMessage, cacheKey, false); resp != nil {
 		// Send cache to client directly.
@@ -569,46 +553,22 @@ func (c *DnsController) dialSend(invokingDepth int, req *udpRequest, data []byte
 
 	// Dial and send.
 	var respMsg *dnsmessage.Msg
-	// defer in a recursive call will delay Close(), thus we Close() before
-	// the next recursive call. However, a connection cannot be closed twice.
-	// We should set a connClosed flag to avoid it.
-	var connClosed bool
 
 	ctxDial, cancel := context.WithTimeout(context.TODO(), consts.DefaultDialTimeout)
 	defer cancel()
 
-	// get forwarder from cache
-	c.dnsForwarderCacheMu.Lock()
-	forwarder, ok := c.dnsForwarderCache[dnsForwarderKey{upstream: upstream.String(), dialArgument: *dialArgument}]
-	if !ok {
-		forwarder, err = newDnsForwarder(upstream, *dialArgument)
-		if err != nil {
-			c.dnsForwarderCacheMu.Unlock()
-			return err
-		}
-		c.dnsForwarderCache[dnsForwarderKey{upstream: upstream.String(), dialArgument: *dialArgument}] = forwarder
-	}
-	c.dnsForwarderCacheMu.Unlock()
-
-	defer func() {
-		if !connClosed {
-			forwarder.Close()
-		}
-	}()
-
+	// 使用新的转发器管理器，避免重复创建
+	forwarder, releaseForwarder, err := GlobalDnsForwarderManager.GetForwarder(upstream, *dialArgument)
 	if err != nil {
 		return err
 	}
+	defer releaseForwarder()
 
 	respMsg, err = forwarder.ForwardDNS(ctxDial, data)
 	if err != nil {
 		return err
 	}
 
-	// Close conn before the recursive call.
-	forwarder.Close()
-	connClosed = true
-
 	// Route response.
 	upstreamIndex, nextUpstream, err := c.routing.ResponseSelect(respMsg, upstream)
 	if err != nil {
diff --git a/control/dns_forwarder_manager.go b/control/dns_forwarder_manager.go
new file mode 100644
index 0000000..07e5c98
--- /dev/null
+++ b/control/dns_forwarder_manager.go
@@ -0,0 +1,165 @@
+/*
+ * SPDX-License-Identifier: AGPL-3.0-only
+ * Copyright (c) 2022-2025, daeuniverse Organization <dae@v2raya.org>
+ */
+
+package control
+
+import (
+	"context"
+	"sync"
+	"time"
+
+	"github.com/daeuniverse/dae/component/dns"
+)
+
+// DnsForwarderManager 管理DNS转发器的生命周期，避免重复创建和资源浪费
+type DnsForwarderManager struct {
+	// 使用sync.Map存储活跃的转发器
+	activeForwarders sync.Map // map[dnsForwarderKey]*forwarderEntry
+	
+	// 清理goroutine控制
+	cleanupInterval time.Duration
+	ctx             context.Context
+	cancel          context.CancelFunc
+}
+
+type forwarderEntry struct {
+	forwarder   DnsForwarder
+	lastUsed    time.Time
+	refCount    int32
+	mu          sync.RWMutex
+}
+
+// GetForwarder 获取或创建DNS转发器，使用引用计数管理生命周期
+func (m *DnsForwarderManager) GetForwarder(upstream *dns.Upstream, dialArg dialArgument) (DnsForwarder, func(), error) {
+	key := dnsForwarderKey{upstream: upstream.String(), dialArgument: dialArg}
+	
+	// 快速路径：尝试获取现有转发器
+	if entryValue, ok := m.activeForwarders.Load(key); ok {
+		entry := entryValue.(*forwarderEntry)
+		entry.mu.Lock()
+		entry.refCount++
+		entry.lastUsed = time.Now()
+		forwarder := entry.forwarder
+		entry.mu.Unlock()
+		
+		// 返回释放函数
+		release := func() {
+			entry.mu.Lock()
+			entry.refCount--
+			entry.mu.Unlock()
+		}
+		
+		return forwarder, release, nil
+	}
+	
+	// 慢路径：需要创建新转发器
+	newForwarder, err := newDnsForwarder(upstream, dialArg)
+	if err != nil {
+		return nil, nil, err
+	}
+	
+	entry := &forwarderEntry{
+		forwarder: newForwarder,
+		lastUsed:  time.Now(),
+		refCount:  1,
+	}
+	
+	// 尝试存储，如果已存在则使用现有的
+	if existingValue, loaded := m.activeForwarders.LoadOrStore(key, entry); loaded {
+		// 有其他goroutine创建了转发器，关闭我们创建的并使用现有的
+		newForwarder.Close()
+		
+		existingEntry := existingValue.(*forwarderEntry)
+		existingEntry.mu.Lock()
+		existingEntry.refCount++
+		existingEntry.lastUsed = time.Now()
+		forwarder := existingEntry.forwarder
+		existingEntry.mu.Unlock()
+		
+		release := func() {
+			existingEntry.mu.Lock()
+			existingEntry.refCount--
+			existingEntry.mu.Unlock()
+		}
+		
+		return forwarder, release, nil
+	}
+	
+	// 成功存储新转发器
+	release := func() {
+		entry.mu.Lock()
+		entry.refCount--
+		entry.mu.Unlock()
+	}
+	
+	return newForwarder, release, nil
+}
+
+// NewDnsForwarderManager 创建新的DNS转发器管理器
+func NewDnsForwarderManager() *DnsForwarderManager {
+	ctx, cancel := context.WithCancel(context.Background())
+	manager := &DnsForwarderManager{
+		cleanupInterval: 5 * time.Minute,
+		ctx:             ctx,
+		cancel:          cancel,
+	}
+	
+	// 启动清理goroutine
+	go manager.cleanupLoop()
+	
+	return manager
+}
+
+// cleanupLoop 定期清理未使用的转发器
+func (m *DnsForwarderManager) cleanupLoop() {
+	ticker := time.NewTicker(m.cleanupInterval)
+	defer ticker.Stop()
+	
+	for {
+		select {
+		case <-m.ctx.Done():
+			return
+		case <-ticker.C:
+			m.cleanup()
+		}
+	}
+}
+
+func (m *DnsForwarderManager) cleanup() {
+	now := time.Now()
+	cutoff := now.Add(-m.cleanupInterval)
+	
+	m.activeForwarders.Range(func(key, value interface{}) bool {
+		entry := value.(*forwarderEntry)
+		entry.mu.RLock()
+		shouldDelete := entry.refCount == 0 && entry.lastUsed.Before(cutoff)
+		forwarder := entry.forwarder
+		entry.mu.RUnlock()
+		
+		if shouldDelete {
+			// 尝试删除并关闭
+			if m.activeForwarders.CompareAndDelete(key, value) {
+				forwarder.Close()
+			}
+		}
+		
+		return true
+	})
+}
+
+// Shutdown 关闭管理器并清理所有转发器
+func (m *DnsForwarderManager) Shutdown() {
+	m.cancel()
+	
+	// 关闭所有活跃的转发器
+	m.activeForwarders.Range(func(key, value interface{}) bool {
+		entry := value.(*forwarderEntry)
+		entry.forwarder.Close()
+		return true
+	})
+}
+
+// 全局DNS转发器管理器实例
+var GlobalDnsForwarderManager = NewDnsForwarderManager()
diff --git a/control/dns_handling_state.go b/control/dns_handling_state.go
new file mode 100644
index 0000000..d3acce6
--- /dev/null
+++ b/control/dns_handling_state.go
@@ -0,0 +1,79 @@
+/*
+ * SPDX-License-Identifier: AGPL-3.0-only
+ * Copyright (c) 2022-2025, daeuniverse Organization <dae@v2raya.org>
+ */
+
+package control
+
+import (
+	"sync"
+	"time"
+)
+
+// SimpleHandlingState 简化的处理状态管理，避免复杂的引用计数
+type SimpleHandlingState struct {
+	done chan struct{}
+	once sync.Once
+}
+
+// Wait 等待处理完成
+func (s *SimpleHandlingState) Wait() {
+	<-s.done
+}
+
+// Complete 标记处理完成
+func (s *SimpleHandlingState) Complete() {
+	s.once.Do(func() {
+		close(s.done)
+	})
+}
+
+// NewSimpleHandlingState 创建新的处理状态
+func NewSimpleHandlingState() *SimpleHandlingState {
+	return &SimpleHandlingState{
+		done: make(chan struct{}),
+	}
+}
+
+// HandlingStateManager 管理DNS处理状态，避免重复请求
+type HandlingStateManager struct {
+	states sync.Map // map[string]*SimpleHandlingState
+}
+
+// GetOrCreateState 获取或创建处理状态
+func (m *HandlingStateManager) GetOrCreateState(key string) (*SimpleHandlingState, bool) {
+	// 尝试获取现有状态
+	if stateValue, ok := m.states.Load(key); ok {
+		return stateValue.(*SimpleHandlingState), false
+	}
+	
+	// 创建新状态
+	newState := NewSimpleHandlingState()
+	
+	// 尝试存储，如果已存在则使用现有的
+	if actualValue, loaded := m.states.LoadOrStore(key, newState); loaded {
+		return actualValue.(*SimpleHandlingState), false
+	}
+	
+	// 成功创建新状态
+	return newState, true
+}
+
+// CompleteAndCleanup 完成处理并清理状态
+func (m *HandlingStateManager) CompleteAndCleanup(key string, state *SimpleHandlingState) {
+	state.Complete()
+	
+	// 延迟清理，给其他等待的goroutine一些时间
+	go func() {
+		time.Sleep(100 * time.Millisecond)
+		m.states.Delete(key)
+	}()
+}
+
+// NewHandlingStateManager 创建新的处理状态管理器
+func NewHandlingStateManager() *HandlingStateManager {
+	return &HandlingStateManager{}
+}
+
+// 全局处理状态管理器
+var GlobalHandlingStateManager = NewHandlingStateManager()
diff --git a/control/udp_endpoint_pool.go b/control/udp_endpoint_pool.go
index 5fd972a..cb87016 100644
--- a/control/udp_endpoint_pool.go
+++ b/control/udp_endpoint_pool.go
@@ -38,22 +38,68 @@ type UdpEndpoint struct {
 }
 
 func (ue *UdpEndpoint) start() {
-	buf := pool.GetFullCap(consts.EthernetMtu)
-	defer pool.Put(buf)
+	// 使用buffered channel实现异步处理
+	const maxPendingPackets = 1000
+	packetChan := make(chan struct {
+		data []byte
+		from netip.AddrPort
+	}, maxPendingPackets)
+	
+	// 启动异步包处理器
+	go func() {
+		for packet := range packetChan {
+			// 异步处理每个包，避免阻塞读取循环
+			go func(data []byte, from netip.AddrPort) {
+				defer pool.Put(data) // 确保释放buffer
+				if err := ue.handler(data, from); err != nil {
+					// 处理错误但不阻塞
+					return
+				}
+			}(packet.data, packet.from)
+		}
+	}()
+	
+	// 高性能读取循环
 	for {
+		buf := pool.GetFullCap(consts.EthernetMtu)
 		n, from, err := ue.conn.ReadFrom(buf[:])
 		if err != nil {
+			pool.Put(buf)
 			break
 		}
+		
+		// 快速重置计时器，减少锁竞争
 		ue.mu.Lock()
-		ue.deadlineTimer.Reset(ue.NatTimeout)
+		if ue.deadlineTimer != nil {
+			ue.deadlineTimer.Reset(ue.NatTimeout)
+		}
 		ue.mu.Unlock()
-		if err = ue.handler(buf[:n], from); err != nil {
-			break
+		
+		// 复制数据到正确大小的buffer
+		data := pool.Get(n)
+		copy(data, buf[:n])
+		pool.Put(buf)
+		
+		// 非阻塞发送到处理器
+		select {
+		case packetChan <- struct {
+			data []byte
+			from netip.AddrPort
+		}{data, from}:
+			// 成功发送到处理队列
+		default:
+			// 队列满了，丢弃包（避免阻塞读取）
+			pool.Put(data)
+			// 可以在这里记录丢包统计
 		}
 	}
+	
+	// 清理
+	close(packetChan)
 	ue.mu.Lock()
-	ue.deadlineTimer.Stop()
+	if ue.deadlineTimer != nil {
+		ue.deadlineTimer.Stop()
+	}
 	ue.mu.Unlock()
 }
 
diff --git a/control/udp_health_monitor.go b/control/udp_health_monitor.go
new file mode 100644
index 0000000..667e863
--- /dev/null
+++ b/control/udp_health_monitor.go
@@ -0,0 +1,165 @@
+/*
+ * SPDX-License-Identifier: AGPL-3.0-only
+ * Copyright (c) 2022-2025, daeuniverse Organization <dae@v2raya.org>
+ */
+
+package control
+
+import (
+	"context"
+	"sync"
+	"sync/atomic"
+	"time"
+)
+
+// UdpHealthMonitor monitors UDP processing health and prevents deadlocks
+type UdpHealthMonitor struct {
+	// 基本指标
+	activeConnections    int64
+	totalPacketsHandled  int64
+	droppedPackets      int64
+	timeoutOccurrences  int64
+	
+	// 控制参数
+	isShuttingDown      int32
+	maxActiveConns      int64
+	healthCheckInterval time.Duration
+	
+	// 监控
+	lastActivity        time.Time
+	mu                 sync.RWMutex
+	ctx                context.Context
+	cancel             context.CancelFunc
+}
+
+// NewUdpHealthMonitor creates a new UDP health monitor
+func NewUdpHealthMonitor() *UdpHealthMonitor {
+	ctx, cancel := context.WithCancel(context.Background())
+	monitor := &UdpHealthMonitor{
+		maxActiveConns:      20000, // 增加最大连接数
+		healthCheckInterval: 30 * time.Second,
+		lastActivity:        time.Now(),
+		ctx:                 ctx,
+		cancel:              cancel,
+	}
+	
+	go monitor.healthCheckLoop()
+	return monitor
+}
+
+// healthCheckLoop runs periodic health checks
+func (m *UdpHealthMonitor) healthCheckLoop() {
+	ticker := time.NewTicker(m.healthCheckInterval)
+	defer ticker.Stop()
+	
+	for {
+		select {
+		case <-m.ctx.Done():
+			return
+		case <-ticker.C:
+			m.performHealthCheck()
+		}
+	}
+}
+
+// performHealthCheck 执行简化的健康检查
+func (m *UdpHealthMonitor) performHealthCheck() {
+	activeConns := atomic.LoadInt64(&m.activeConnections)
+	totalPackets := atomic.LoadInt64(&m.totalPacketsHandled)
+	droppedPackets := atomic.LoadInt64(&m.droppedPackets)
+	timeouts := atomic.LoadInt64(&m.timeoutOccurrences)
+	
+	// 简单的日志记录（如果需要的话）
+	_ = activeConns
+	_ = totalPackets
+	_ = droppedPackets
+	_ = timeouts
+	
+	// 重置计数器防止溢出
+	if totalPackets > 10000000 { // 1000万包后重置
+		atomic.StoreInt64(&m.totalPacketsHandled, 0)
+		atomic.StoreInt64(&m.droppedPackets, 0)
+		atomic.StoreInt64(&m.timeoutOccurrences, 0)
+	}
+}
+
+// RegisterConnection registers a new UDP connection
+func (m *UdpHealthMonitor) RegisterConnection() bool {
+	if atomic.LoadInt32(&m.isShuttingDown) != 0 {
+		return false
+	}
+	
+	activeConns := atomic.AddInt64(&m.activeConnections, 1)
+	if activeConns > m.maxActiveConns {
+		atomic.AddInt64(&m.activeConnections, -1)
+		atomic.AddInt64(&m.droppedPackets, 1)
+		return false
+	}
+	
+	m.mu.Lock()
+	m.lastActivity = time.Now()
+	m.mu.Unlock()
+	
+	return true
+}
+
+// UnregisterConnection unregisters a UDP connection
+func (m *UdpHealthMonitor) UnregisterConnection() {
+	atomic.AddInt64(&m.activeConnections, -1)
+}
+
+// RecordPacketHandled records a successfully handled packet
+func (m *UdpHealthMonitor) RecordPacketHandled() {
+	atomic.AddInt64(&m.totalPacketsHandled, 1)
+	
+	m.mu.Lock()
+	m.lastActivity = time.Now()
+	m.mu.Unlock()
+}
+
+// RecordTimeout records a timeout occurrence
+func (m *UdpHealthMonitor) RecordTimeout() {
+	atomic.AddInt64(&m.timeoutOccurrences, 1)
+}
+
+// IsHealthy returns true if the system is in a healthy state
+func (m *UdpHealthMonitor) IsHealthy() bool {
+	activeConns := atomic.LoadInt64(&m.activeConnections)
+	timeouts := atomic.LoadInt64(&m.timeoutOccurrences)
+	totalPackets := atomic.LoadInt64(&m.totalPacketsHandled)
+	
+	// 基本健康检查
+	if activeConns > m.maxActiveConns*9/10 { // 90% 容量
+		return false
+	}
+	
+	// 超时率检查
+	if totalPackets > 1000 {
+		timeoutRate := float64(timeouts) / float64(totalPackets)
+		if timeoutRate > 0.05 { // 5% 超时率阈值
+			return false
+		}
+	}
+	
+	return true
+}
+
+// Shutdown shuts down the health monitor
+func (m *UdpHealthMonitor) Shutdown() {
+	atomic.StoreInt32(&m.isShuttingDown, 1)
+	m.cancel()
+}
+
+// GetStats returns current statistics
+func (m *UdpHealthMonitor) GetStats() map[string]int64 {
+	return map[string]int64{
+		"active_connections":    atomic.LoadInt64(&m.activeConnections),
+		"total_packets_handled": atomic.LoadInt64(&m.totalPacketsHandled),
+		"dropped_packets":       atomic.LoadInt64(&m.droppedPackets),
+		"timeout_occurrences":   atomic.LoadInt64(&m.timeoutOccurrences),
+		"max_active_conns":      m.maxActiveConns,
+	}
+}
+
+// Global UDP health monitor instance
+var DefaultUdpHealthMonitor = NewUdpHealthMonitor()
diff --git a/control/udp_task_pool.go b/control/udp_task_pool.go
index 08b02d7..09d8cb4 100644
--- a/control/udp_task_pool.go
+++ b/control/udp_task_pool.go
@@ -11,7 +11,11 @@ import (
 	"time"
 )
 
-const UdpTaskQueueLength = 128
+const (
+	UdpTaskQueueLength = 512  // 增加队列容量以支持更高并发
+	MaxUdpQueues       = 5000 // 增加最大队列数
+	UdpTaskTimeout     = 100 * time.Millisecond // 极短超时时间
+)
 
 type UdpTask = func()
 
@@ -27,22 +31,57 @@ type UdpTaskQueue struct {
 }
 
 func (q *UdpTaskQueue) convoy() {
+	defer close(q.closed)
+	
 	for {
 		select {
 		case <-q.ctx.Done():
-			close(q.closed)
+			// 清空剩余任务
+			q.drainRemainingTasks()
 			return
+			
 		case task := <-q.ch:
-			task()
-			q.timer.Reset(q.agingTime)
+			// 立即异步执行任务，不等待完成
+			go q.executeTaskAsync(task)
+			
+			// 重置老化定时器
+			if q.timer != nil {
+				q.timer.Reset(q.agingTime)
+			}
+		}
+	}
+}
+
+// executeTaskAsync 异步执行单个任务
+func (q *UdpTaskQueue) executeTaskAsync(task UdpTask) {
+	defer func() {
+		if r := recover(); r != nil {
+			// 记录panic但不影响其他任务
+		}
+	}()
+	
+	if task != nil {
+		task()
+	}
+}
+
+// drainRemainingTasks 清空剩余任务
+func (q *UdpTaskQueue) drainRemainingTasks() {
+	for {
+		select {
+		case task := <-q.ch:
+			// 异步执行剩余任务
+			go q.executeTaskAsync(task)
+		default:
+			return
 		}
 	}
 }
 
 type UdpTaskPool struct {
 	queueChPool sync.Pool
-	// mu protects m
-	mu sync.Mutex
+	// 使用RWMutex提高读取性能
+	mu sync.RWMutex
 	m  map[string]*UdpTaskQueue
 }
 
@@ -51,7 +90,7 @@ func NewUdpTaskPool() *UdpTaskPool {
 		queueChPool: sync.Pool{New: func() any {
 			return make(chan UdpTask, UdpTaskQueueLength)
 		}},
-		mu: sync.Mutex{},
+		mu: sync.RWMutex{},
 		m:  map[string]*UdpTaskQueue{},
 	}
 	return p
@@ -59,40 +98,123 @@ func NewUdpTaskPool() *UdpTaskPool {
 
 // EmitTask: Make sure packets with the same key (4 tuples) will be sent in order.
 func (p *UdpTaskPool) EmitTask(key string, task UdpTask) {
+	if task == nil {
+		return
+	}
+
+	// 快速健康检查
+	if !DefaultUdpHealthMonitor.RegisterConnection() {
+		return
+	}
+	defer DefaultUdpHealthMonitor.UnregisterConnection()
+
+	// 尝试使用读锁快速查找现有队列
+	p.mu.RLock()
+	q, exists := p.m[key]
+	queueCount := len(p.m)
+	p.mu.RUnlock()
+
+	if exists {
+		// 队列已存在，直接提交任务
+		p.submitTaskToQueue(q, task)
+		return
+	}
+
+	// 需要创建新队列，使用写锁
 	p.mu.Lock()
-	q, ok := p.m[key]
-	if !ok {
-		ch := p.queueChPool.Get().(chan UdpTask)
-		ctx, cancel := context.WithCancel(context.Background())
-		q = &UdpTaskQueue{
-			key:       key,
-			p:         p,
-			ch:        ch,
-			timer:     nil,
-			agingTime: DefaultNatTimeout,
-			ctx:       ctx,
-			closed:    make(chan struct{}),
-		}
-		q.timer = time.AfterFunc(q.agingTime, func() {
-			// if timer executed, there should no task in queue.
-			// q.closed should not blocking things.
-			p.mu.Lock()
-			cancel()
-			delete(p.m, key)
-			p.mu.Unlock()
-			<-q.closed
-			if len(ch) == 0 { // Otherwise let it be GCed
-				p.queueChPool.Put(ch)
+	defer p.mu.Unlock()
+
+	// 双重检查
+	if q, exists := p.m[key]; exists {
+		p.submitTaskToQueue(q, task)
+		return
+	}
+
+	// 限制队列数量
+	if queueCount >= MaxUdpQueues {
+		DefaultUdpHealthMonitor.RecordTimeout()
+		return
+	}
+
+	// 创建新队列
+	ch := p.queueChPool.Get().(chan UdpTask)
+	ctx, cancel := context.WithCancel(context.Background())
+	q = &UdpTaskQueue{
+		key:       key,
+		p:         p,
+		ch:        ch,
+		timer:     nil,
+		agingTime: DefaultNatTimeout,
+		ctx:       ctx,
+		closed:    make(chan struct{}),
+	}
+
+	q.timer = time.AfterFunc(q.agingTime, func() {
+		p.cleanupQueue(key, q, cancel, ch)
+	})
+
+	p.m[key] = q
+	go q.convoy()
+
+	// 提交任务到新创建的队列
+	p.submitTaskToQueue(q, task)
+}
+
+// submitTaskToQueue 提交任务到指定队列（极简版本）
+func (p *UdpTaskPool) submitTaskToQueue(q *UdpTaskQueue, task UdpTask) {
+	// 包装任务以增加健康监控
+	wrappedTask := func() {
+		defer func() {
+			DefaultUdpHealthMonitor.RecordPacketHandled()
+			if r := recover(); r != nil {
+				// 记录panic但继续
 			}
-		})
-		p.m[key] = q
-		go q.convoy()
+		}()
+		task()
 	}
-	p.mu.Unlock()
-	// if task cannot be executed within 180s(DefaultNatTimeout), GC may be triggered, so skip the task when GC occurs
+
+	// 极速任务提交 - 非阻塞模式
 	select {
-	case q.ch <- task:
+	case q.ch <- wrappedTask:
+		// 任务成功排队
 	case <-q.ctx.Done():
+		// 上下文已取消
+		DefaultUdpHealthMonitor.RecordTimeout()
+	default:
+		// 队列已满，异步重试一次
+		go func() {
+			select {
+			case q.ch <- wrappedTask:
+				// 重试成功
+			case <-q.ctx.Done():
+				DefaultUdpHealthMonitor.RecordTimeout()
+			case <-time.After(UdpTaskTimeout):
+				DefaultUdpHealthMonitor.RecordTimeout()
+			}
+		}()
+	}
+}
+
+// cleanupQueue 清理队列
+func (p *UdpTaskPool) cleanupQueue(key string, q *UdpTaskQueue, cancel context.CancelFunc, ch chan UdpTask) {
+	p.mu.Lock()
+	cancel()
+	delete(p.m, key)
+	p.mu.Unlock()
+
+	// 等待清理完成，带超时
+	select {
+	case <-q.closed:
+	case <-time.After(1 * time.Second):
+		// 强制清理
+	}
+
+	// 回收通道
+	if len(ch) == 0 {
+		for len(ch) > 0 {
+			<-ch
+		}
+		p.queueChPool.Put(ch)
 	}
 }
 
